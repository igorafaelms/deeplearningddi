{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SIIM_2019_Hands_on_Deep_Learning_Extended.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/igorafaelms/deeplearningddi/blob/master/SIIM_2019_Hands_on_Deep_Learning_Extended.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uH4B_VCqjKy3",
        "colab_type": "text"
      },
      "source": [
        "![SIIM](https://siim.org/resource/resmgr/siim2019/siim19_banner-2000.png)\n",
        "\n",
        "# SIIM 2019 Annual Meeting\n",
        "\n",
        "# Hands-on - Deep Learning for Intracranial Hemorrhage Detection\n",
        "\n",
        "\n",
        "\n",
        "**Developed by:**\n",
        "\n",
        "Luciano M. Prevedello, MD, MPH (luciano.prevedello@osumc.edu)\n",
        "\n",
        "Felipe Campos Kitamura, MD, MSc (kitamura.felipe@gmail.com)\n",
        "\n",
        "Igor Santos, MD (igor.msantos@fidi.org.br)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WbAUq1KmksS9",
        "colab_type": "text"
      },
      "source": [
        "**Step by step:**\n",
        "\n",
        "All the process will be demonstrated with Python 3 running on Google Colaboratory. Please make sure you have GPU enabled under notebook settings before you proceed.\n",
        "\n",
        "**There are 3 training sets:**\n",
        "\n",
        "- Dataset 0 comprises 60 normal and 6 hemorrhage head CT images.\n",
        "\n",
        "- Dataset 1 comprises 33 normal and 33 hemorrhage head CT images.\n",
        "\n",
        "- Dataset 2 comprises 60 normal and 60 hemorrhage head CT images.\n",
        "\n",
        "Validation and Test sets have 20 normal and 20 hemorrhage each, except for dataset 0 (20 normal and 2 hemorrhages).\n",
        "\n",
        "For each specific task we will import specific libraries."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zKhWVEba2o8k",
        "colab_type": "text"
      },
      "source": [
        "##Dataset Download\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wQb6GaikkTRb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Installing dependencies\n",
        "\n",
        "!pip3 install keras-vis\n",
        "!pip3 install imgaug==0.2.5\n",
        "!pip3 install scipy==1.2.1\n",
        "\n",
        "print ('\\033[1m' + 'Finished! Go to next step.')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lWIB0LS3Ud6u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#First of all, we are going to download the zip files with the images to this instance of Google's Colaboratory\n",
        "\n",
        "!wget https://github.com/kitamura-felipe/deeplearning_head_ct_demo/blob/master/Allcases.zip?raw=true \n",
        "!wget https://github.com/kitamura-felipe/deeplearning_head_ct_demo/blob/master/33_33.zip?raw=true\n",
        "!wget https://github.com/kitamura-felipe/deeplearning_head_ct_demo/blob/master/60_6.zip?raw=true\n",
        "  \n",
        "print ('\\033[1m' + 'Ready for next step!')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cWmvZke5D3wu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Let's unzip them\n",
        "\n",
        "!unzip Allcases.zip?raw=true -d /\n",
        "!unzip 33_33.zip?raw=true -d /\n",
        "!unzip 60_6.zip?raw=true -d /\n",
        "\n",
        "print ('\\033[1m' + 'Ready for next step!')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CuH4VJ7LNoVf",
        "colab_type": "text"
      },
      "source": [
        "##Experiment 1\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5DgBd6adcj5s",
        "colab_type": "text"
      },
      "source": [
        "### 1.1 - Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H0Kcsc8RPBLd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Importing libraries for arrays (NumPy), Pre-processing (Keras) and plotting images (Matplotlib)\n",
        "\n",
        "import numpy as np\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# It is important to set a random seed in order to have reproducbility of training results between different users\n",
        "\n",
        "from numpy.random import seed\n",
        "seed(123)\n",
        "from tensorflow import set_random_seed\n",
        "set_random_seed(123)\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Dimensions which our images will be resized for the input. All of them must have the same size\n",
        "\n",
        "img_width, img_height = 150, 150\n",
        "\n",
        "# We split the data between 60/10/30% for training/validation/test sets \n",
        "# We choose which directories must be used\n",
        "\n",
        "validation_data_dir = '/All cases/Validation/'\n",
        "nb_validation_samples = 40\n",
        "\n",
        "test_data_dir = '/All cases/Test/'\n",
        "nb_test_samples = 40\n",
        "\n",
        "dataset = 0\n",
        "\n",
        "if dataset == 0:\n",
        "  train_data_dir = '/60+6/Training/'\n",
        "  nb_train_samples = 66\n",
        "  test_data_dir = '/60+6/Test/'\n",
        "  nb_test_samples = 40\n",
        "  validation_data_dir = '/60+6/Validation/'\n",
        "  nb_validation_samples = 40\n",
        "elif dataset == 1:\n",
        "  train_data_dir = '/33+33/Training/'\n",
        "  nb_train_samples = 66\n",
        "else:\n",
        "  train_data_dir = '/All cases/Training/'\n",
        "  nb_train_samples = 120\n",
        "\n",
        "\n",
        "# For generator we need to give these two hyperparameters\n",
        "epochs = 40\n",
        "batch_size = 5\n",
        "\n",
        "\n",
        "# This is the augmentation configuration we will use for training\n",
        "dataaug = 0\n",
        "\n",
        "if dataaug == 0:\n",
        "  print(\"Data Aug Off\")\n",
        "  train_datagen = ImageDataGenerator(\n",
        "      rescale=1. / 255) # normalization\n",
        "elif dataaug == 2:\n",
        "  print(\"Data Aug ON 2\")\n",
        "  train_datagen = ImageDataGenerator(\n",
        "      rescale=1. / 255, # normalization\n",
        "      width_shift_range=1.0,\n",
        "      height_shift_range=1.0,\n",
        "      shear_range=0.2,\n",
        "      zoom_range=1.0,\n",
        "      rotation_range=90,\n",
        "      horizontal_flip=True)  \n",
        "else:\n",
        "  print(\"Data Aug ON 1\")\n",
        "  train_datagen = ImageDataGenerator(\n",
        "      rescale=1. / 255, # normalization\n",
        "      width_shift_range=0.1,\n",
        "      height_shift_range=0.1,\n",
        "      shear_range=0.02,\n",
        "      zoom_range=0.05, \n",
        "      rotation_range=10,\n",
        "      horizontal_flip=True)\n",
        "\n",
        "# This is the augmentation configuration we will use for validation:\n",
        "val_datagen = ImageDataGenerator(rescale=1. / 255) # normalization\n",
        "\n",
        "test_datagen = ImageDataGenerator(rescale=1. / 255) # normalization\n",
        "\n",
        "print(\"Training set:\")\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    train_data_dir,\n",
        "    target_size=(img_width, img_height),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='binary')\n",
        "\n",
        "print(\"Validation set:\")\n",
        "validation_generator = val_datagen.flow_from_directory(\n",
        "    validation_data_dir,\n",
        "    target_size=(img_width, img_height),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='binary')\n",
        "\n",
        "print(\"Test set:\")\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "    test_data_dir,\n",
        "    target_size=(img_width, img_height),\n",
        "    batch_size=nb_test_samples,\n",
        "    class_mode='binary', shuffle = False)\n",
        "\n",
        "# Let's plot the first 4 generator outputs, defining the positive cases as Label = True and negatives as Label = False \n",
        "\n",
        "x,y = train_generator.next()\n",
        "labley = y==0\n",
        "for i in range(0, 4):\n",
        " plt.subplot(220 + 1 + i).grid(False)\n",
        " plt.imshow(x[i], cmap=plt.get_cmap('gray'))\n",
        " plt.title(\"\\nLable:{}\".format(labley[i]))\n",
        " plt.axis('off')\n",
        "\n",
        "# show the plot\n",
        "plt.show()\n",
        "\n",
        "print ('\\033[1m' + 'Ready for next step!')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wd_lvXxSc9hu",
        "colab_type": "text"
      },
      "source": [
        "### 1.2 - Model Compilation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dbFtqnHNPX_y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Now we import keras modules for creating a linear stack of layer (Sequential)\n",
        "# Then we import the specific layers we want to use in our model\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from keras.layers import Activation, Dropout, Flatten, Dense, GlobalAveragePooling2D\n",
        "from keras import backend as K\n",
        "\n",
        "# We define the size of the input of the Neural Net\n",
        "\n",
        "img_width, img_height = 150, 150\n",
        "\n",
        "# we could use 1 color channel instead of 3\n",
        "# we are using 3 channels for simplifying transfer learning implementation\n",
        "\n",
        "input_shape = (img_width, img_height, 3) # width, height, # of color channels\n",
        "\n",
        "SIIM_custom_model = Sequential()\n",
        "\n",
        "# Below we have the first Convolutional Layer\n",
        "\n",
        "SIIM_custom_model.add(Conv2D(32, (3, 3), input_shape=input_shape, kernel_initializer=\"he_normal\"))\n",
        "SIIM_custom_model.add(Activation('relu'))\n",
        "\n",
        "# We then add a MaxPool layer, which will reduce the size of the output of the first conv layer in 75%.\n",
        "# This is performed to avoid an exagerated increase in the number of parameters of the network.\n",
        "# Don't worry if you do not understand in detail each one of these operations right now. Try to focus on the big picture.\n",
        "\n",
        "SIIM_custom_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "# We will add more convolutional layers, followed by MaxPool layers\n",
        "\n",
        "SIIM_custom_model.add(Conv2D(32, (3, 3), kernel_initializer=\"he_normal\"))\n",
        "SIIM_custom_model.add(Activation('relu'))\n",
        "SIIM_custom_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "SIIM_custom_model.add(Conv2D(64, (3, 3), kernel_initializer=\"he_normal\"))\n",
        "SIIM_custom_model.add(Activation('relu'))\n",
        "SIIM_custom_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "\n",
        "# Finally, we will add two dense layers, or 'Fully Connected Layers'.\n",
        "# These layers are classical neural nets, without convolutions.\n",
        "\n",
        "SIIM_custom_model.add(Flatten())\n",
        "SIIM_custom_model.add(Dense(32, kernel_initializer=\"he_normal\"))\n",
        "SIIM_custom_model.add(Activation('relu'))\n",
        "\n",
        "# Dropout is an overfitting reduction technique.\n",
        "\n",
        "SIIM_custom_model.add(Dropout(0.2))\n",
        "\n",
        "# Now, we will set the output o the network.\n",
        "# The Dense function has the argument \"1\" because the net output is the hematoma x non-hematoma classification\n",
        "\n",
        "SIIM_custom_model.add(Dense(1))\n",
        "\n",
        "# The output is either 0 or 1 and this can be obtained with a sigmoid function.\n",
        "\n",
        "SIIM_custom_model.add(Activation('sigmoid'))\n",
        "\n",
        "# Let's compile the network.\n",
        "\n",
        "SIIM_custom_model.compile(loss='binary_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "print ('\\033[1m' + 'Ready for next step!')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sM9BebVwdRdn",
        "colab_type": "text"
      },
      "source": [
        "### 1.3 - Training and Validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jjEM6sNoPY-a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Now, let's train our SIIM Neural Net:\n",
        "\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "checkpointer = ModelCheckpoint(filepath='best_SIIM_custom_model.hdf5', monitor='val_loss',\n",
        "                               verbose=1, save_best_only=True)\n",
        "\n",
        "hist = SIIM_custom_model.fit_generator(\n",
        "    train_generator,\n",
        "    steps_per_epoch= 120 // batch_size,\n",
        "    epochs=epochs,\n",
        "    validation_data=validation_generator,\n",
        "    validation_steps=nb_validation_samples // batch_size,\n",
        "    callbacks=[checkpointer])\n",
        "\n",
        "print ('\\033[1m' + 'Ready for next step!')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0bKmjRZPPreg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Plotting the loss function\n",
        "\n",
        "plt.plot(hist.history['loss'], 'b-', label='train loss')\n",
        "plt.plot(hist.history['val_loss'], 'r-', label='val loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "plt.plot(hist.history['acc'], 'b-', label='train accuracy')\n",
        "plt.plot(hist.history['val_acc'], 'r-', label='val accuracy')\n",
        "plt.ylabel('acc')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "print('\\033[1m' + \"Best Validation Accuracy: \" + str(hist.history['val_acc'][np.argmin(hist.history['val_loss'])]))\n",
        "print(\"  \")\n",
        "print ('\\033[1m' + 'Ready for next step!')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pgj2NICudbdo",
        "colab_type": "text"
      },
      "source": [
        "### 1.4 - Performance Metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1KIfrJW7PsVC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import load_model\n",
        "\n",
        "# Loading the best model\n",
        "\n",
        "best_model = load_model('best_SIIM_custom_model.hdf5')\n",
        "\n",
        "print ('\\033[1m' + 'Ready for next step!')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ETtC_x2EP66Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Defining a function to plot a confusion matrix.\n",
        "\n",
        "# from https://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.utils.multiclass import unique_labels\n",
        "\n",
        "def plot_confusion_matrix(y_true, y_pred, classes,\n",
        "                          normalize=False,\n",
        "                          title=None,\n",
        "                          cmap=plt.cm.Blues):\n",
        "    \"\"\"\n",
        "    This function prints and plots the confusion matrix.\n",
        "    Normalization can be applied by setting `normalize=True`.\n",
        "    \"\"\"\n",
        "    if not title:\n",
        "        if normalize:\n",
        "            title = 'Normalized confusion matrix'\n",
        "        else:\n",
        "            title = 'Confusion matrix, without normalization'\n",
        "\n",
        "    # Compute confusion matrix\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    # Only use the labels that appear in the data\n",
        "    #classes = classes[unique_labels(y_true, y_pred)]\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "        #print(\"Normalized confusion matrix\")\n",
        "    #else:\n",
        "        #print('Confusion matrix, without normalization')\n",
        "\n",
        "    #print(cm)\n",
        "\n",
        "    fig, ax = plt.subplots()\n",
        "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    ax.figure.colorbar(im, ax=ax)\n",
        "    # We want to show all ticks...\n",
        "    ax.set(xticks=np.arange(cm.shape[1]),\n",
        "           yticks=np.arange(cm.shape[0]),\n",
        "           # ... and label them with the respective list entries\n",
        "           xticklabels=classes, yticklabels=classes,\n",
        "           title=title,\n",
        "           ylabel='Ground-truth label',\n",
        "           xlabel='Predicted label')\n",
        "\n",
        "    # Rotate the tick labels and set their alignment.\n",
        "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
        "             rotation_mode=\"anchor\")\n",
        "\n",
        "    # Loop over data dimensions and create text annotations.\n",
        "    fmt = '.2f' if normalize else 'd'\n",
        "    thresh = cm.max() / 2.\n",
        "    for i in range(cm.shape[0]):\n",
        "        for j in range(cm.shape[1]):\n",
        "            ax.text(j, i, format(cm[i, j], fmt),\n",
        "                    ha=\"center\", va=\"center\",\n",
        "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "    fig.tight_layout()\n",
        "    return ax\n",
        "\n",
        "  \n",
        "X, Y = test_generator.next() # Get the X (images) and Y (labels) of the test set\n",
        "\n",
        "labels_pred = 1 - best_model.predict(X) #predict the output from X\n",
        "\n",
        "labels_pred = labels_pred > labels_pred.mean() #predictions greater than mean are set to 1, those lesser than or equal to mean are set to 0.\n",
        "\n",
        "labels_test =  Y==0\n",
        "\n",
        "np.set_printoptions(precision=2)\n",
        "\n",
        "# Plot non-normalized confusion matrix\n",
        "plot_confusion_matrix(labels_test, labels_pred.astype('int'), classes=['Normal','Hematoma'], normalize=False,\n",
        "                      title='Confusion matrix, without normalization')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OofafSKGQUYr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Plotting the ROC curve with the AUC\n",
        "\n",
        "labels_pred = 1 - best_model.predict(X) # predict again to get the original sigmoid output [0,1]\n",
        "\n",
        "from sklearn import metrics\n",
        "\n",
        "fpr, tpr, thresholds = metrics.roc_curve(labels_test, labels_pred)\n",
        "roc_auc = metrics.auc(fpr, tpr)\n",
        "\n",
        "plt.figure()\n",
        "lw = 2\n",
        "plt.plot(fpr, tpr, color='darkorange',\n",
        "         lw=lw, label='ROC curve (area = %0.2f)' % roc_auc)\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
        "plt.xlim([-0.01, 1.0])\n",
        "plt.ylim([0.0, 1.01])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver operating characteristic example')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()\n",
        "\n",
        "labels_pred = labels_pred > labels_pred.mean() #predictions greater than mean are set to 1, those lesser than or equal to mean are set to 0.\n",
        "\n",
        "f1_score = metrics.f1_score(labels_test, labels_pred, labels=None, pos_label=1, average='binary', sample_weight=None)\n",
        "\n",
        "accuracy = metrics.accuracy_score(labels_test, labels_pred)\n",
        "\n",
        "print(\"Accuracy: \" + str(accuracy))\n",
        "\n",
        "print(\"F1 Score: \" + str(f1_score))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xjphAoSXOXqD",
        "colab_type": "text"
      },
      "source": [
        "##Experiment 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pe6Q5eheRHU1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#reset runtimes\n",
        "\n",
        "import os\n",
        "os.kill(os.getpid(), 9)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XL9kF9zvdwKe",
        "colab_type": "text"
      },
      "source": [
        "### 2.1 Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mnmATO6kRNjn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Importing libraries for arrays (NumPy), Pre-processing (Keras) and plotting images (Matplotlib)\n",
        "\n",
        "import numpy as np\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# It is important to set a random seed in order to have reproducbility of training results between different users\n",
        "\n",
        "from numpy.random import seed\n",
        "seed(123)\n",
        "from tensorflow import set_random_seed\n",
        "set_random_seed(123)\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Dimensions which our images will be resized for the input. All of them must have the same size\n",
        "\n",
        "img_width, img_height = 150, 150\n",
        "\n",
        "# We split the data between 60/10/30% for training/validation/test sets \n",
        "# We choose which directories must be used\n",
        "\n",
        "validation_data_dir = '/All cases/Validation/'\n",
        "nb_validation_samples = 40\n",
        "\n",
        "test_data_dir = '/All cases/Test/'\n",
        "nb_test_samples = 40\n",
        "\n",
        "dataset = 1\n",
        "\n",
        "if dataset == 0:\n",
        "  train_data_dir = '/60+6/Training/'\n",
        "  nb_train_samples = 66\n",
        "  test_data_dir = '/60+6/Test/'\n",
        "  nb_test_samples = 40\n",
        "  validation_data_dir = '/60+6/Validation/'\n",
        "  nb_validation_samples = 40\n",
        "elif dataset == 1:\n",
        "  train_data_dir = '/33+33/Training/'\n",
        "  nb_train_samples = 66\n",
        "else:\n",
        "  train_data_dir = '/All cases/Training/'\n",
        "  nb_train_samples = 120\n",
        "\n",
        "\n",
        "# For generator we need to give these two hyperparameters\n",
        "epochs = 40\n",
        "batch_size = 5\n",
        "\n",
        "\n",
        "# This is the augmentation configuration we will use for training\n",
        "dataaug = 0\n",
        "\n",
        "if dataaug == 0:\n",
        "  print(\"Data Aug Off\")\n",
        "  train_datagen = ImageDataGenerator(\n",
        "      rescale=1. / 255) # normalization\n",
        "elif dataaug == 2:\n",
        "  print(\"Data Aug ON 2\")\n",
        "  train_datagen = ImageDataGenerator(\n",
        "      rescale=1. / 255, # normalization\n",
        "      width_shift_range=1.0,\n",
        "      height_shift_range=1.0,\n",
        "      shear_range=0.2,\n",
        "      zoom_range=1.0,\n",
        "      rotation_range=90,\n",
        "      horizontal_flip=True)  \n",
        "else:\n",
        "  print(\"Data Aug ON 1\")\n",
        "  train_datagen = ImageDataGenerator(\n",
        "      rescale=1. / 255, # normalization\n",
        "      width_shift_range=0.1,\n",
        "      height_shift_range=0.1,\n",
        "      shear_range=0.02,\n",
        "      zoom_range=0.05, \n",
        "      rotation_range=10,\n",
        "      horizontal_flip=True)\n",
        "\n",
        "# This is the augmentation configuration we will use for validation:\n",
        "val_datagen = ImageDataGenerator(rescale=1. / 255) # normalization\n",
        "\n",
        "test_datagen = ImageDataGenerator(rescale=1. / 255) # normalization\n",
        "\n",
        "print(\"Training set:\")\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    train_data_dir,\n",
        "    target_size=(img_width, img_height),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='binary')\n",
        "\n",
        "print(\"Validation set:\")\n",
        "validation_generator = val_datagen.flow_from_directory(\n",
        "    validation_data_dir,\n",
        "    target_size=(img_width, img_height),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='binary')\n",
        "\n",
        "print(\"Test set:\")\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "    test_data_dir,\n",
        "    target_size=(img_width, img_height),\n",
        "    batch_size=nb_test_samples,\n",
        "    class_mode='binary', shuffle = False)\n",
        "\n",
        "# Let's plot the first 4 generator outputs, defining the positive cases as Label = True and negatives as Label = False \n",
        "\n",
        "x,y = train_generator.next()\n",
        "labley = y==0\n",
        "for i in range(0, 4):\n",
        " plt.subplot(220 + 1 + i).grid(False)\n",
        " plt.imshow(x[i], cmap=plt.get_cmap('gray'))\n",
        " plt.title(\"\\nLable:{}\".format(labley[i]))\n",
        " plt.axis('off')\n",
        "\n",
        "# show the plot\n",
        "plt.show()\n",
        "\n",
        "print ('\\033[1m' + 'Ready for next step!')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Njhet4NEd3Gn",
        "colab_type": "text"
      },
      "source": [
        "### 2.2 - Model Compilation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yq0KYs8CROis",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Now we import keras modules for creating a linear stack of layer (Sequential)\n",
        "# Then we import the specific layers we want to use in our model\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from keras.layers import Activation, Dropout, Flatten, Dense, GlobalAveragePooling2D\n",
        "from keras import backend as K\n",
        "\n",
        "# We define the size of the input of the Neural Net\n",
        "\n",
        "img_width, img_height = 150, 150\n",
        "\n",
        "# we could use 1 color channel instead of 3\n",
        "# we are using 3 channels for simplifying transfer learning implementation\n",
        "\n",
        "input_shape = (img_width, img_height, 3) # width, height, # of color channels\n",
        "\n",
        "SIIM_custom_model = Sequential()\n",
        "\n",
        "# Below we have the first Convolutional Layer\n",
        "\n",
        "SIIM_custom_model.add(Conv2D(32, (3, 3), input_shape=input_shape, kernel_initializer=\"he_normal\"))\n",
        "SIIM_custom_model.add(Activation('relu'))\n",
        "\n",
        "# We then add a MaxPool layer, which will reduce the size of the output of the first conv layer in 75%.\n",
        "# This is performed to avoid an exagerated increase in the number of parameters of the network.\n",
        "# Don't worry if you do not understand in detail each one of these operations right now. Try to focus on the big picture.\n",
        "\n",
        "SIIM_custom_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "# We will add more convolutional layers, followed by MaxPool layers\n",
        "\n",
        "SIIM_custom_model.add(Conv2D(32, (3, 3), kernel_initializer=\"he_normal\"))\n",
        "SIIM_custom_model.add(Activation('relu'))\n",
        "SIIM_custom_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "SIIM_custom_model.add(Conv2D(64, (3, 3), kernel_initializer=\"he_normal\"))\n",
        "SIIM_custom_model.add(Activation('relu'))\n",
        "SIIM_custom_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "\n",
        "# Finally, we will add two dense layers, or 'Fully Connected Layers'.\n",
        "# These layers are classical neural nets, without convolutions.\n",
        "\n",
        "SIIM_custom_model.add(Flatten())\n",
        "SIIM_custom_model.add(Dense(32, kernel_initializer=\"he_normal\"))\n",
        "SIIM_custom_model.add(Activation('relu'))\n",
        "\n",
        "# Dropout is an overfitting reduction technique.\n",
        "\n",
        "SIIM_custom_model.add(Dropout(0.2))\n",
        "\n",
        "# Now, we will set the output o the network.\n",
        "# The Dense function has the argument \"1\" because the net output is the hematoma x non-hematoma classification\n",
        "\n",
        "SIIM_custom_model.add(Dense(1))\n",
        "\n",
        "# The output is either 0 or 1 and this can be obtained with a sigmoid function.\n",
        "\n",
        "SIIM_custom_model.add(Activation('sigmoid'))\n",
        "\n",
        "# Let's compile the network.\n",
        "\n",
        "SIIM_custom_model.compile(loss='binary_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "print ('\\033[1m' + 'Ready for next step!')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v910Nd-Zd-7P",
        "colab_type": "text"
      },
      "source": [
        "### 2.3 - Training and Validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qvpjTD8URPGH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Now, let's train our SIIM Neural Net:\n",
        "\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "checkpointer = ModelCheckpoint(filepath='best_SIIM_custom_model.hdf5', monitor='val_loss',\n",
        "                               verbose=1, save_best_only=True)\n",
        "\n",
        "hist = SIIM_custom_model.fit_generator(\n",
        "    train_generator,\n",
        "    steps_per_epoch= 120 // batch_size,\n",
        "    epochs=epochs,\n",
        "    validation_data=validation_generator,\n",
        "    validation_steps=nb_validation_samples // batch_size,\n",
        "    callbacks=[checkpointer])\n",
        "\n",
        "print ('\\033[1m' + 'Ready for next step!')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ZXPcrt2RPt6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Plotting the loss function\n",
        "\n",
        "plt.plot(hist.history['loss'], 'b-', label='train loss')\n",
        "plt.plot(hist.history['val_loss'], 'r-', label='val loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "plt.plot(hist.history['acc'], 'b-', label='train accuracy')\n",
        "plt.plot(hist.history['val_acc'], 'r-', label='val accuracy')\n",
        "plt.ylabel('acc')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "print('\\033[1m' + \"Best Validation Accuracy: \" + str(hist.history['val_acc'][np.argmin(hist.history['val_loss'])]))\n",
        "print(\"  \")\n",
        "\n",
        "print ('\\033[1m' + 'Ready for next step!')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2TaHP1EgePN8",
        "colab_type": "text"
      },
      "source": [
        "### 2.4 - Performance Metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1NbOnKKeRQfs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import load_model\n",
        "\n",
        "# Loading the best model\n",
        "\n",
        "best_model = load_model('best_SIIM_custom_model.hdf5')\n",
        "\n",
        "print ('\\033[1m' + 'Ready for next step!')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5h1jC3l5RRYV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Defining a function to plot a confusion matrix.\n",
        "\n",
        "# from https://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.utils.multiclass import unique_labels\n",
        "\n",
        "def plot_confusion_matrix(y_true, y_pred, classes,\n",
        "                          normalize=False,\n",
        "                          title=None,\n",
        "                          cmap=plt.cm.Blues):\n",
        "    \"\"\"\n",
        "    This function prints and plots the confusion matrix.\n",
        "    Normalization can be applied by setting `normalize=True`.\n",
        "    \"\"\"\n",
        "    if not title:\n",
        "        if normalize:\n",
        "            title = 'Normalized confusion matrix'\n",
        "        else:\n",
        "            title = 'Confusion matrix, without normalization'\n",
        "\n",
        "    # Compute confusion matrix\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    # Only use the labels that appear in the data\n",
        "    #classes = classes[unique_labels(y_true, y_pred)]\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "        #print(\"Normalized confusion matrix\")\n",
        "    #else:\n",
        "        #print('Confusion matrix, without normalization')\n",
        "\n",
        "    #print(cm)\n",
        "\n",
        "    fig, ax = plt.subplots()\n",
        "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    ax.figure.colorbar(im, ax=ax)\n",
        "    # We want to show all ticks...\n",
        "    ax.set(xticks=np.arange(cm.shape[1]),\n",
        "           yticks=np.arange(cm.shape[0]),\n",
        "           # ... and label them with the respective list entries\n",
        "           xticklabels=classes, yticklabels=classes,\n",
        "           title=title,\n",
        "           ylabel='Ground-truth label',\n",
        "           xlabel='Predicted label')\n",
        "\n",
        "    # Rotate the tick labels and set their alignment.\n",
        "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
        "             rotation_mode=\"anchor\")\n",
        "\n",
        "    # Loop over data dimensions and create text annotations.\n",
        "    fmt = '.2f' if normalize else 'd'\n",
        "    thresh = cm.max() / 2.\n",
        "    for i in range(cm.shape[0]):\n",
        "        for j in range(cm.shape[1]):\n",
        "            ax.text(j, i, format(cm[i, j], fmt),\n",
        "                    ha=\"center\", va=\"center\",\n",
        "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "    fig.tight_layout()\n",
        "    return ax\n",
        "\n",
        "  \n",
        "X, Y = test_generator.next() # Get the X (images) and Y (labels) of the test set\n",
        "\n",
        "labels_pred = 1 - best_model.predict(X) #predict the output from X\n",
        "\n",
        "labels_pred = labels_pred > labels_pred.mean() #predictions greater than mean are set to 1, those lesser than or equal to mean are set to 0.\n",
        "\n",
        "labels_test =  Y==0\n",
        "\n",
        "np.set_printoptions(precision=2)\n",
        "\n",
        "# Plot non-normalized confusion matrix\n",
        "plot_confusion_matrix(labels_test, labels_pred.astype('int'), classes=['Normal','Hematoma'], normalize=False,\n",
        "                      title='Confusion matrix, without normalization')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KB56mMgPRUnD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Plotting the ROC curve with the AUC\n",
        "\n",
        "labels_pred = 1 - best_model.predict(X) # predict again to get the original sigmoid output [0,1]\n",
        "\n",
        "from sklearn import metrics\n",
        "\n",
        "fpr, tpr, thresholds = metrics.roc_curve(labels_test, labels_pred)\n",
        "roc_auc = metrics.auc(fpr, tpr)\n",
        "\n",
        "plt.figure()\n",
        "lw = 2\n",
        "plt.plot(fpr, tpr, color='darkorange',\n",
        "         lw=lw, label='ROC curve (area = %0.2f)' % roc_auc)\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
        "plt.xlim([-0.01, 1.0])\n",
        "plt.ylim([0.0, 1.01])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver operating characteristic example')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()\n",
        "\n",
        "labels_pred = labels_pred > labels_pred.mean() #predictions greater than mean are set to 1, those lesser than or equal to mean are set to 0.\n",
        "\n",
        "f1_score = metrics.f1_score(labels_test, labels_pred, labels=None, pos_label=1, average='binary', sample_weight=None)\n",
        "\n",
        "accuracy = metrics.accuracy_score(labels_test, labels_pred)\n",
        "\n",
        "print(\"Accuracy: \" + str(accuracy))\n",
        "\n",
        "print(\"F1 Score: \" + str(f1_score))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4W6-wqhtOZzY",
        "colab_type": "text"
      },
      "source": [
        "##Experiment 3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AjGhTGVBSTWq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#reset runtimes\n",
        "\n",
        "import os\n",
        "os.kill(os.getpid(), 9)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZNRLeR4seWus",
        "colab_type": "text"
      },
      "source": [
        "### 3.1 - Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nvbhggqqST7a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Importing libraries for arrays (NumPy), Pre-processing (Keras) and plotting images (Matplotlib)\n",
        "\n",
        "import numpy as np\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# It is important to set a random seed in order to have reproducbility of training results between different users\n",
        "\n",
        "from numpy.random import seed\n",
        "seed(123)\n",
        "from tensorflow import set_random_seed\n",
        "set_random_seed(123)\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Dimensions which our images will be resized for the input. All of them must have the same size\n",
        "\n",
        "img_width, img_height = 150, 150\n",
        "\n",
        "# We split the data between 60/10/30% for training/validation/test sets \n",
        "# We choose which directories must be used\n",
        "\n",
        "validation_data_dir = '/All cases/Validation/'\n",
        "nb_validation_samples = 40\n",
        "\n",
        "test_data_dir = '/All cases/Test/'\n",
        "nb_test_samples = 40\n",
        "\n",
        "dataset = 1\n",
        "\n",
        "if dataset == 0:\n",
        "  train_data_dir = '/60+6/Training/'\n",
        "  nb_train_samples = 66\n",
        "  test_data_dir = '/60+6/Test/'\n",
        "  nb_test_samples = 40\n",
        "  validation_data_dir = '/60+6/Validation/'\n",
        "  nb_validation_samples = 40\n",
        "elif dataset == 1:\n",
        "  train_data_dir = '/33+33/Training/'\n",
        "  nb_train_samples = 66\n",
        "else:\n",
        "  train_data_dir = '/All cases/Training/'\n",
        "  nb_train_samples = 120\n",
        "\n",
        "\n",
        "# For generator we need to give these two hyperparameters\n",
        "epochs = 40\n",
        "batch_size = 5\n",
        "\n",
        "\n",
        "# This is the augmentation configuration we will use for training\n",
        "dataaug = 0\n",
        "\n",
        "if dataaug == 0:\n",
        "  print(\"Data Aug Off\")\n",
        "  train_datagen = ImageDataGenerator(\n",
        "      rescale=1. / 255) # normalization\n",
        "elif dataaug == 2:\n",
        "  print(\"Data Aug ON 2\")\n",
        "  train_datagen = ImageDataGenerator(\n",
        "      rescale=1. / 255, # normalization\n",
        "      width_shift_range=1.0,\n",
        "      height_shift_range=1.0,\n",
        "      shear_range=0.2,\n",
        "      zoom_range=1.0,\n",
        "      rotation_range=90,\n",
        "      horizontal_flip=True)  \n",
        "else:\n",
        "  print(\"Data Aug ON 1\")\n",
        "  train_datagen = ImageDataGenerator(\n",
        "      rescale=1. / 255, # normalization\n",
        "      width_shift_range=0.1,\n",
        "      height_shift_range=0.1,\n",
        "      shear_range=0.02,\n",
        "      zoom_range=0.05, \n",
        "      rotation_range=10,\n",
        "      horizontal_flip=True)\n",
        "\n",
        "# This is the augmentation configuration we will use for validation:\n",
        "val_datagen = ImageDataGenerator(rescale=1. / 255) # normalization\n",
        "\n",
        "test_datagen = ImageDataGenerator(rescale=1. / 255) # normalization\n",
        "\n",
        "print(\"Training set:\")\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    train_data_dir,\n",
        "    target_size=(img_width, img_height),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='binary')\n",
        "\n",
        "print(\"Validation set:\")\n",
        "validation_generator = val_datagen.flow_from_directory(\n",
        "    validation_data_dir,\n",
        "    target_size=(img_width, img_height),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='binary')\n",
        "\n",
        "print(\"Test set:\")\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "    test_data_dir,\n",
        "    target_size=(img_width, img_height),\n",
        "    batch_size=nb_test_samples,\n",
        "    class_mode='binary', shuffle = False)\n",
        "\n",
        "# Let's plot the first 4 generator outputs, defining the positive cases as Label = True and negatives as Label = False \n",
        "\n",
        "x,y = train_generator.next()\n",
        "labley = y==0\n",
        "for i in range(0, 4):\n",
        " plt.subplot(220 + 1 + i).grid(False)\n",
        " plt.imshow(x[i], cmap=plt.get_cmap('gray'))\n",
        " plt.title(\"\\nLable:{}\".format(labley[i]))\n",
        " plt.axis('off')\n",
        "\n",
        "# show the plot\n",
        "plt.show()\n",
        "\n",
        "print ('\\033[1m' + 'Ready for next step!')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p7lNfrq9eexn",
        "colab_type": "text"
      },
      "source": [
        "### 3.2 - Transfer Learning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q5CMQWuMSUg1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# We can improve our results using transfer learning\n",
        "\n",
        "from keras.applications.vgg16 import VGG16\n",
        "base_model = VGG16(weights='imagenet', include_top=False, input_shape=(img_width, img_height, 3))\n",
        "\n",
        "print ('\\033[1m' + 'Ready for next step!')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x3pltT8ZSU-R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Let's edit the last layers of VGG16 to use it in our solution\n",
        "\n",
        "from keras.layers import Activation, Dropout, Flatten, Dense, GlobalAveragePooling2D\n",
        "from keras.models import Model\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras import optimizers\n",
        "\n",
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "\n",
        "# Only for version 2\n",
        "x = Dense(1024, activation='relu')(x)\n",
        "\n",
        "# And a logistic layer\n",
        "predictions = Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "SIIM_Net= Model(inputs=base_model.input, outputs=predictions)\n",
        "\n",
        "print ('\\033[1m' + 'Ready for next step!')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vR_svq8zSVxy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# We can try using a different optimizer as well\n",
        "\n",
        "sgd = optimizers.SGD(lr=0.0001, decay=1e-6, momentum=0.9, nesterov=True)\n",
        "\n",
        "SIIM_Net.compile(optimizer=sgd, loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "print ('\\033[1m' + 'Ready for next step!')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dqIv3hV_eprZ",
        "colab_type": "text"
      },
      "source": [
        "### 3.3 - Training and Validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SCQAIJ4_SWgz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Time to train it\n",
        "\n",
        "checkpointer = ModelCheckpoint(filepath='Best_model.hdf5', monitor='val_loss',\n",
        "                               verbose=1, save_best_only=True)\n",
        "\n",
        "hist = SIIM_Net.fit_generator(\n",
        "            train_generator,\n",
        "            steps_per_epoch=nb_train_samples // batch_size,\n",
        "            epochs=epochs,\n",
        "            validation_data=validation_generator,\n",
        "            validation_steps=nb_validation_samples // batch_size,\n",
        "            callbacks=[checkpointer])\n",
        "\n",
        "print ('\\033[1m' + 'Ready for next step!')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VTVQDkjYS8Kp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Plotting the loss function\n",
        "\n",
        "plt.plot(hist.history['loss'], 'b-', label='train loss')\n",
        "plt.plot(hist.history['val_loss'], 'r-', label='val loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "plt.plot(hist.history['acc'], 'b-', label='train accuracy')\n",
        "plt.plot(hist.history['val_acc'], 'r-', label='val accuracy')\n",
        "plt.ylabel('acc')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "print('\\033[1m' + \"Best Validation Accuracy: \" + str(hist.history['val_acc'][np.argmin(hist.history['val_loss'])]))\n",
        "print(\"  \")\n",
        "\n",
        "print ('\\033[1m' + 'Ready for next step!')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WSUX4qbSeyoL",
        "colab_type": "text"
      },
      "source": [
        "### 3.4 - Performance Metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SNNWbb3gS7RS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import load_model\n",
        "\n",
        "#Loading the best model\n",
        "\n",
        "best_model = load_model('Best_model.hdf5')\n",
        "\n",
        "print ('\\033[1m' + 'Ready for next step!')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rFSj7q4JS6Sj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Defining a function to plot a confusion matrix.\n",
        "\n",
        "# from https://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.utils.multiclass import unique_labels\n",
        "\n",
        "def plot_confusion_matrix(y_true, y_pred, classes,\n",
        "                          normalize=False,\n",
        "                          title=None,\n",
        "                          cmap=plt.cm.Blues):\n",
        "    \"\"\"\n",
        "    This function prints and plots the confusion matrix.\n",
        "    Normalization can be applied by setting `normalize=True`.\n",
        "    \"\"\"\n",
        "    if not title:\n",
        "        if normalize:\n",
        "            title = 'Normalized confusion matrix'\n",
        "        else:\n",
        "            title = 'Confusion matrix, without normalization'\n",
        "\n",
        "    # Compute confusion matrix\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    # Only use the labels that appear in the data\n",
        "    #classes = classes[unique_labels(y_true, y_pred)]\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "        #print(\"Normalized confusion matrix\")\n",
        "    #else:\n",
        "        #print('Confusion matrix, without normalization')\n",
        "\n",
        "    #print(cm)\n",
        "\n",
        "    fig, ax = plt.subplots()\n",
        "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    ax.figure.colorbar(im, ax=ax)\n",
        "    # We want to show all ticks...\n",
        "    ax.set(xticks=np.arange(cm.shape[1]),\n",
        "           yticks=np.arange(cm.shape[0]),\n",
        "           # ... and label them with the respective list entries\n",
        "           xticklabels=classes, yticklabels=classes,\n",
        "           title=title,\n",
        "           ylabel='Ground-truth label',\n",
        "           xlabel='Predicted label')\n",
        "\n",
        "    # Rotate the tick labels and set their alignment.\n",
        "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
        "             rotation_mode=\"anchor\")\n",
        "\n",
        "    # Loop over data dimensions and create text annotations.\n",
        "    fmt = '.2f' if normalize else 'd'\n",
        "    thresh = cm.max() / 2.\n",
        "    for i in range(cm.shape[0]):\n",
        "        for j in range(cm.shape[1]):\n",
        "            ax.text(j, i, format(cm[i, j], fmt),\n",
        "                    ha=\"center\", va=\"center\",\n",
        "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "    fig.tight_layout()\n",
        "    return ax\n",
        "\n",
        "X, Y = test_generator.next() # Get the X (images) and Y (labels) of the test set\n",
        "\n",
        "labels_pred = best_model.predict(X) #predict the output from X\n",
        "\n",
        "labels_pred = labels_pred > labels_pred.mean() #predictions greater than mean are set to 1, those lesser than or equal to mean are set to 0.\n",
        "\n",
        "labels_test = Y\n",
        "\n",
        "np.set_printoptions(precision=2)\n",
        "\n",
        "# Plot non-normalized confusion matrix\n",
        "plot_confusion_matrix(labels_test, labels_pred.astype('int'), classes=['Normal','Hematoma'], normalize=False,\n",
        "                      title='Confusion matrix, without normalization')\n",
        "\n",
        "print ('\\033[1m' + 'Ready for next step!')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mRUv6lk9USI8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Plotting the ROC curve with the AUC\n",
        "\n",
        "labels_pred = best_model.predict(X) # predict again to get the original sigmoid output [0,1]\n",
        "\n",
        "from sklearn import metrics\n",
        "\n",
        "fpr, tpr, thresholds = metrics.roc_curve(labels_test, labels_pred)\n",
        "roc_auc = metrics.auc(fpr, tpr)\n",
        "\n",
        "plt.figure()\n",
        "lw = 2\n",
        "plt.plot(fpr, tpr, color='darkorange',\n",
        "         lw=lw, label='ROC curve (area = %0.2f)' % roc_auc)\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
        "plt.xlim([-0.01, 1.0])\n",
        "plt.ylim([0.0, 1.01])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver operating characteristic example')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()\n",
        "\n",
        "labels_pred = labels_pred > labels_pred.mean() #predictions greater than mean are set to 1, those lesser than or equal to mean are set to 0.\n",
        "\n",
        "f1_score = metrics.f1_score(labels_test, labels_pred, labels=None, pos_label=1, average='binary', sample_weight=None)\n",
        "\n",
        "accuracy = metrics.accuracy_score(labels_test, labels_pred)\n",
        "\n",
        "print(\"Accuracy: \" + str(accuracy))\n",
        "\n",
        "print(\"F1 Score: \" + str(f1_score))\n",
        "\n",
        "print ('\\033[1m' + 'Ready for next step!')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-hao52FIOcOP",
        "colab_type": "text"
      },
      "source": [
        "##Experiment 4"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wNQL37uDU_qJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#reset runtimes\n",
        "\n",
        "import os\n",
        "os.kill(os.getpid(), 9)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g9IX9s8He6gi",
        "colab_type": "text"
      },
      "source": [
        "### 4.1 - Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "awqH22bqVABb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Importing libraries for arrays (NumPy), Pre-processing (Keras) and plotting images (Matplotlib)\n",
        "\n",
        "import numpy as np\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# It is important to set a random seed in order to have reproducbility of training results between different users\n",
        "\n",
        "from numpy.random import seed\n",
        "seed(123)\n",
        "from tensorflow import set_random_seed\n",
        "set_random_seed(123)\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Dimensions which our images will be resized for the input. All of them must have the same size\n",
        "\n",
        "img_width, img_height = 150, 150\n",
        "\n",
        "# We split the data between 60/10/30% for training/validation/test sets \n",
        "# We choose which directories must be used\n",
        "\n",
        "validation_data_dir = '/All cases/Validation/'\n",
        "nb_validation_samples = 40\n",
        "\n",
        "test_data_dir = '/All cases/Test/'\n",
        "nb_test_samples = 40\n",
        "\n",
        "dataset = 1\n",
        "\n",
        "if dataset == 0:\n",
        "  train_data_dir = '/60+6/Training/'\n",
        "  nb_train_samples = 66\n",
        "  test_data_dir = '/60+6/Test/'\n",
        "  nb_test_samples = 40\n",
        "  validation_data_dir = '/60+6/Validation/'\n",
        "  nb_validation_samples = 40\n",
        "elif dataset == 1:\n",
        "  train_data_dir = '/33+33/Training/'\n",
        "  nb_train_samples = 66\n",
        "else:\n",
        "  train_data_dir = '/All cases/Training/'\n",
        "  nb_train_samples = 120\n",
        "\n",
        "\n",
        "# For generator we need to give these two hyperparameters\n",
        "epochs = 40\n",
        "batch_size = 5\n",
        "\n",
        "\n",
        "# This is the augmentation configuration we will use for training\n",
        "dataaug = 1\n",
        "\n",
        "if dataaug == 0:\n",
        "  print(\"Data Aug Off\")\n",
        "  train_datagen = ImageDataGenerator(\n",
        "      rescale=1. / 255) # normalization\n",
        "elif dataaug == 2:\n",
        "  print(\"Data Aug ON 2\")\n",
        "  train_datagen = ImageDataGenerator(\n",
        "      rescale=1. / 255, # normalization\n",
        "      width_shift_range=1.0,\n",
        "      height_shift_range=1.0,\n",
        "      shear_range=0.2,\n",
        "      zoom_range=1.0,\n",
        "      rotation_range=90,\n",
        "      horizontal_flip=True)  \n",
        "else:\n",
        "  print(\"Data Aug ON 1\")\n",
        "  train_datagen = ImageDataGenerator(\n",
        "      rescale=1. / 255, # normalization\n",
        "      width_shift_range=0.1,\n",
        "      height_shift_range=0.1,\n",
        "      shear_range=0.02,\n",
        "      zoom_range=0.05, \n",
        "      rotation_range=10,\n",
        "      horizontal_flip=True)\n",
        "\n",
        "# This is the augmentation configuration we will use for validation:\n",
        "val_datagen = ImageDataGenerator(rescale=1. / 255) # normalization\n",
        "\n",
        "test_datagen = ImageDataGenerator(rescale=1. / 255) # normalization\n",
        "\n",
        "print(\"Training set:\")\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    train_data_dir,\n",
        "    target_size=(img_width, img_height),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='binary')\n",
        "\n",
        "print(\"Validation set:\")\n",
        "validation_generator = val_datagen.flow_from_directory(\n",
        "    validation_data_dir,\n",
        "    target_size=(img_width, img_height),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='binary')\n",
        "\n",
        "print(\"Test set:\")\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "    test_data_dir,\n",
        "    target_size=(img_width, img_height),\n",
        "    batch_size=nb_test_samples,\n",
        "    class_mode='binary', shuffle = False)\n",
        "\n",
        "# Let's plot the first 4 generator outputs, defining the positive cases as Label = True and negatives as Label = False \n",
        "\n",
        "x,y = train_generator.next()\n",
        "labley = y==0\n",
        "for i in range(0, 4):\n",
        " plt.subplot(220 + 1 + i).grid(False)\n",
        " plt.imshow(x[i], cmap=plt.get_cmap('gray'))\n",
        " plt.title(\"\\nLable:{}\".format(labley[i]))\n",
        " plt.axis('off')\n",
        "\n",
        "# show the plot\n",
        "plt.show()\n",
        "\n",
        "print ('\\033[1m' + 'Ready for next step!')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KlDRVo15fCiK",
        "colab_type": "text"
      },
      "source": [
        "### 4.2 - Transfer Learning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vDKXMWh1VAVu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# We can improve our results using transfer learning\n",
        "\n",
        "from keras.applications.vgg16 import VGG16\n",
        "base_model = VGG16(weights='imagenet', include_top=False, input_shape=(img_width, img_height, 3))\n",
        "\n",
        "print ('\\033[1m' + 'Ready for next step!')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-shoKsuLVAqZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Let's edit the last layers of VGG16 to use it in our solution\n",
        "\n",
        "from keras.layers import Activation, Dropout, Flatten, Dense, GlobalAveragePooling2D\n",
        "from keras.models import Model\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras import optimizers\n",
        "\n",
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "\n",
        "# Only for version 2\n",
        "x = Dense(1024, activation='relu')(x)\n",
        "\n",
        "# And a logistic layer\n",
        "predictions = Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "SIIM_Net= Model(inputs=base_model.input, outputs=predictions)\n",
        "\n",
        "print ('\\033[1m' + 'Ready for next step!')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PaDpBuvKVA-c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# We can try using a different optimizer as well\n",
        "\n",
        "sgd = optimizers.SGD(lr=0.0001, decay=1e-6, momentum=0.9, nesterov=True)\n",
        "\n",
        "SIIM_Net.compile(optimizer=sgd, loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "print ('\\033[1m' + 'Ready for next step!')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hc3t5nrxfJn9",
        "colab_type": "text"
      },
      "source": [
        "### 4.3 - Training and Validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z1gkkoe1VBWd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Time to train it\n",
        "\n",
        "checkpointer = ModelCheckpoint(filepath='Best_model.hdf5', monitor='val_loss',\n",
        "                               verbose=1, save_best_only=True)\n",
        "\n",
        "hist = SIIM_Net.fit_generator(\n",
        "            train_generator,\n",
        "            steps_per_epoch=nb_train_samples // batch_size,\n",
        "            epochs=epochs,\n",
        "            validation_data=validation_generator,\n",
        "            validation_steps=nb_validation_samples // batch_size,\n",
        "            callbacks=[checkpointer])\n",
        "\n",
        "print ('\\033[1m' + 'Ready for next step!')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EShLpQAmVBtc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Plotting the loss function\n",
        "\n",
        "plt.plot(hist.history['loss'], 'b-', label='train loss')\n",
        "plt.plot(hist.history['val_loss'], 'r-', label='val loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "plt.plot(hist.history['acc'], 'b-', label='train accuracy')\n",
        "plt.plot(hist.history['val_acc'], 'r-', label='val accuracy')\n",
        "plt.ylabel('acc')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "print('\\033[1m' + \"Best Validation Accuracy: \" + str(hist.history['val_acc'][np.argmin(hist.history['val_loss'])]))\n",
        "print(\"  \")\n",
        "\n",
        "\n",
        "print ('\\033[1m' + 'Ready for next step!')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8YatEaL8fRmQ",
        "colab_type": "text"
      },
      "source": [
        "### 4.4 - Performance Metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NBll9PtBVCGR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import load_model\n",
        "\n",
        "#Loading the best model\n",
        "\n",
        "best_model = load_model('Best_model.hdf5')\n",
        "\n",
        "print ('\\033[1m' + 'Ready for next step!')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vdlb9bpsVCe0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Defining a function to plot a confusion matrix.\n",
        "\n",
        "# from https://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.utils.multiclass import unique_labels\n",
        "\n",
        "def plot_confusion_matrix(y_true, y_pred, classes,\n",
        "                          normalize=False,\n",
        "                          title=None,\n",
        "                          cmap=plt.cm.Blues):\n",
        "    \"\"\"\n",
        "    This function prints and plots the confusion matrix.\n",
        "    Normalization can be applied by setting `normalize=True`.\n",
        "    \"\"\"\n",
        "    if not title:\n",
        "        if normalize:\n",
        "            title = 'Normalized confusion matrix'\n",
        "        else:\n",
        "            title = 'Confusion matrix, without normalization'\n",
        "\n",
        "    # Compute confusion matrix\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    # Only use the labels that appear in the data\n",
        "    #classes = classes[unique_labels(y_true, y_pred)]\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "        #print(\"Normalized confusion matrix\")\n",
        "    #else:\n",
        "        #print('Confusion matrix, without normalization')\n",
        "\n",
        "    #print(cm)\n",
        "\n",
        "    fig, ax = plt.subplots()\n",
        "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    ax.figure.colorbar(im, ax=ax)\n",
        "    # We want to show all ticks...\n",
        "    ax.set(xticks=np.arange(cm.shape[1]),\n",
        "           yticks=np.arange(cm.shape[0]),\n",
        "           # ... and label them with the respective list entries\n",
        "           xticklabels=classes, yticklabels=classes,\n",
        "           title=title,\n",
        "           ylabel='Ground-truth label',\n",
        "           xlabel='Predicted label')\n",
        "\n",
        "    # Rotate the tick labels and set their alignment.\n",
        "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
        "             rotation_mode=\"anchor\")\n",
        "\n",
        "    # Loop over data dimensions and create text annotations.\n",
        "    fmt = '.2f' if normalize else 'd'\n",
        "    thresh = cm.max() / 2.\n",
        "    for i in range(cm.shape[0]):\n",
        "        for j in range(cm.shape[1]):\n",
        "            ax.text(j, i, format(cm[i, j], fmt),\n",
        "                    ha=\"center\", va=\"center\",\n",
        "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "    fig.tight_layout()\n",
        "    return ax\n",
        "\n",
        "X, Y = test_generator.next() # Get the X (images) and Y (labels) of the test set\n",
        "\n",
        "labels_pred = best_model.predict(X) #predict the output from X\n",
        "\n",
        "labels_pred = labels_pred > labels_pred.mean() #predictions greater than mean are set to 1, those lesser than or equal to mean are set to 0.\n",
        "\n",
        "labels_test = Y\n",
        "\n",
        "np.set_printoptions(precision=2)\n",
        "\n",
        "# Plot non-normalized confusion matrix\n",
        "plot_confusion_matrix(labels_test, labels_pred.astype('int'), classes=['Normal','Hematoma'], normalize=False,\n",
        "                      title='Confusion matrix, without normalization')\n",
        "\n",
        "print ('\\033[1m' + 'Ready for next step!')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q5BismFfVC3G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Plotting the ROC curve with the AUC\n",
        "\n",
        "labels_pred = best_model.predict(X) # predict again to get the original sigmoid output [0,1]\n",
        "\n",
        "from sklearn import metrics\n",
        "\n",
        "fpr, tpr, thresholds = metrics.roc_curve(labels_test, labels_pred)\n",
        "roc_auc = metrics.auc(fpr, tpr)\n",
        "\n",
        "plt.figure()\n",
        "lw = 2\n",
        "plt.plot(fpr, tpr, color='darkorange',\n",
        "         lw=lw, label='ROC curve (area = %0.2f)' % roc_auc)\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
        "plt.xlim([-0.01, 1.0])\n",
        "plt.ylim([0.0, 1.01])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver operating characteristic example')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()\n",
        "\n",
        "labels_pred = labels_pred > labels_pred.mean() #predictions greater than mean are set to 1, those lesser than or equal to mean are set to 0.\n",
        "\n",
        "f1_score = metrics.f1_score(labels_test, labels_pred, labels=None, pos_label=1, average='binary', sample_weight=None)\n",
        "\n",
        "accuracy = metrics.accuracy_score(labels_test, labels_pred)\n",
        "\n",
        "print(\"Accuracy: \" + str(accuracy))\n",
        "\n",
        "print(\"F1 Score: \" + str(f1_score))\n",
        "\n",
        "print ('\\033[1m' + 'Ready for next step!')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "99Ajr-buOcrp",
        "colab_type": "text"
      },
      "source": [
        "##Experiment 5"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cdQ6303rOhb9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#reset runtimes\n",
        "\n",
        "import os\n",
        "os.kill(os.getpid(), 9)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oOTYeJBRfasa",
        "colab_type": "text"
      },
      "source": [
        "### 5.1 - Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WTiwSerlOh-Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Importing libraries for arrays (NumPy), Pre-processing (Keras) and plotting images (Matplotlib)\n",
        "\n",
        "import numpy as np\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# It is important to set a random seed in order to have reproducbility of training results between different users\n",
        "\n",
        "from numpy.random import seed\n",
        "seed(123)\n",
        "from tensorflow import set_random_seed\n",
        "set_random_seed(123)\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Dimensions which our images will be resized for the input. All of them must have the same size\n",
        "\n",
        "img_width, img_height = 150, 150\n",
        "\n",
        "# We split the data between 60/10/30% for training/validation/test sets \n",
        "# We choose which directories must be used\n",
        "\n",
        "validation_data_dir = '/All cases/Validation/'\n",
        "nb_validation_samples = 40\n",
        "\n",
        "test_data_dir = '/All cases/Test/'\n",
        "nb_test_samples = 40\n",
        "\n",
        "dataset = 2\n",
        "\n",
        "if dataset == 0:\n",
        "  train_data_dir = '/60+6/Training/'\n",
        "  nb_train_samples = 66\n",
        "  test_data_dir = '/60+6/Test/'\n",
        "  nb_test_samples = 40\n",
        "  validation_data_dir = '/60+6/Validation/'\n",
        "  nb_validation_samples = 40\n",
        "elif dataset == 1:\n",
        "  train_data_dir = '/33+33/Training/'\n",
        "  nb_train_samples = 66\n",
        "else:\n",
        "  train_data_dir = '/All cases/Training/'\n",
        "  nb_train_samples = 120\n",
        "\n",
        "\n",
        "# For generator we need to give these two hyperparameters\n",
        "epochs = 40\n",
        "batch_size = 5\n",
        "\n",
        "\n",
        "# This is the augmentation configuration we will use for training\n",
        "dataaug = 1\n",
        "\n",
        "if dataaug == 0:\n",
        "  print(\"Data Aug Off\")\n",
        "  train_datagen = ImageDataGenerator(\n",
        "      rescale=1. / 255) # normalization\n",
        "elif dataaug == 2:\n",
        "  print(\"Data Aug ON 2\")\n",
        "  train_datagen = ImageDataGenerator(\n",
        "      rescale=1. / 255, # normalization\n",
        "      width_shift_range=1.0,\n",
        "      height_shift_range=1.0,\n",
        "      shear_range=0.2,\n",
        "      zoom_range=1.0,\n",
        "      rotation_range=90,\n",
        "      horizontal_flip=True)  \n",
        "else:\n",
        "  print(\"Data Aug ON 1\")\n",
        "  train_datagen = ImageDataGenerator(\n",
        "      rescale=1. / 255, # normalization\n",
        "      width_shift_range=0.1,\n",
        "      height_shift_range=0.1,\n",
        "      shear_range=0.02,\n",
        "      zoom_range=0.05, \n",
        "      rotation_range=10,\n",
        "      horizontal_flip=True)\n",
        "\n",
        "# This is the augmentation configuration we will use for validation:\n",
        "val_datagen = ImageDataGenerator(rescale=1. / 255) # normalization\n",
        "\n",
        "test_datagen = ImageDataGenerator(rescale=1. / 255) # normalization\n",
        "\n",
        "print(\"Training set:\")\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    train_data_dir,\n",
        "    target_size=(img_width, img_height),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='binary')\n",
        "\n",
        "print(\"Validation set:\")\n",
        "validation_generator = val_datagen.flow_from_directory(\n",
        "    validation_data_dir,\n",
        "    target_size=(img_width, img_height),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='binary')\n",
        "\n",
        "print(\"Test set:\")\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "    test_data_dir,\n",
        "    target_size=(img_width, img_height),\n",
        "    batch_size=nb_test_samples,\n",
        "    class_mode='binary', shuffle = False)\n",
        "\n",
        "# Let's plot the first 4 generator outputs, defining the positive cases as Label = True and negatives as Label = False \n",
        "\n",
        "x,y = train_generator.next()\n",
        "labley = y==0\n",
        "for i in range(0, 4):\n",
        " plt.subplot(220 + 1 + i).grid(False)\n",
        " plt.imshow(x[i], cmap=plt.get_cmap('gray'))\n",
        " plt.title(\"\\nLable:{}\".format(labley[i]))\n",
        " plt.axis('off')\n",
        "\n",
        "# show the plot\n",
        "plt.show()\n",
        "\n",
        "print ('\\033[1m' + 'Ready for next step!')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U4j34Jh4fkVG",
        "colab_type": "text"
      },
      "source": [
        "### 5.2 - Transfer Learning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y_Ok9aT2Oiad",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# We can improve our results using transfer learning\n",
        "\n",
        "from keras.applications.vgg16 import VGG16\n",
        "base_model = VGG16(weights='imagenet', include_top=False, input_shape=(img_width, img_height, 3))\n",
        "\n",
        "print ('\\033[1m' + 'Ready for next step!')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dhiMh7mDOiti",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Let's edit the last layers of VGG16 to use it in our solution\n",
        "\n",
        "from keras.layers import Activation, Dropout, Flatten, Dense, GlobalAveragePooling2D\n",
        "from keras.models import Model\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras import optimizers\n",
        "\n",
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "\n",
        "# Only for version 2\n",
        "x = Dense(1024, activation='relu')(x)\n",
        "\n",
        "# And a logistic layer\n",
        "predictions = Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "SIIM_Net= Model(inputs=base_model.input, outputs=predictions)\n",
        "\n",
        "print ('\\033[1m' + 'Ready for next step!')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bmkl8xtdOhxk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# We can try using a different optimizer as well\n",
        "\n",
        "sgd = optimizers.SGD(lr=0.0001, decay=1e-6, momentum=0.9, nesterov=True)\n",
        "\n",
        "SIIM_Net.compile(optimizer=sgd, loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "print ('\\033[1m' + 'Ready for next step!')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uGQiw6fMfzIE",
        "colab_type": "text"
      },
      "source": [
        "### 5.3 - Training and Validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ahSbEXg8XCwA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Time to train it\n",
        "\n",
        "checkpointer = ModelCheckpoint(filepath='Best_model.hdf5', monitor='val_loss',\n",
        "                               verbose=1, save_best_only=True)\n",
        "\n",
        "hist = SIIM_Net.fit_generator(\n",
        "            train_generator,\n",
        "            steps_per_epoch=nb_train_samples // batch_size,\n",
        "            epochs=epochs,\n",
        "            validation_data=validation_generator,\n",
        "            validation_steps=nb_validation_samples // batch_size,\n",
        "            callbacks=[checkpointer])\n",
        "\n",
        "print ('\\033[1m' + 'Ready for next step!')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nLr_swt3XDJu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Plotting the loss function\n",
        "\n",
        "plt.plot(hist.history['loss'], 'b-', label='train loss')\n",
        "plt.plot(hist.history['val_loss'], 'r-', label='val loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "plt.plot(hist.history['acc'], 'b-', label='train accuracy')\n",
        "plt.plot(hist.history['val_acc'], 'r-', label='val accuracy')\n",
        "plt.ylabel('acc')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "print('\\033[1m' + \"Best Validation Accuracy: \" + str(hist.history['val_acc'][np.argmin(hist.history['val_loss'])]))\n",
        "print(\"  \")\n",
        "\n",
        "\n",
        "print ('\\033[1m' + 'Ready for next step!')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tuDNkwq7f-7I",
        "colab_type": "text"
      },
      "source": [
        "### 5.4 - Performance Metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F-MkQ-ijXDr8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import load_model\n",
        "\n",
        "#Loading the best model\n",
        "\n",
        "best_model = load_model('Best_model.hdf5')\n",
        "\n",
        "print ('\\033[1m' + 'Ready for next step!')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IjoGH2SXXWjR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Defining a function to plot a confusion matrix.\n",
        "\n",
        "# from https://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.utils.multiclass import unique_labels\n",
        "\n",
        "def plot_confusion_matrix(y_true, y_pred, classes,\n",
        "                          normalize=False,\n",
        "                          title=None,\n",
        "                          cmap=plt.cm.Blues):\n",
        "    \"\"\"\n",
        "    This function prints and plots the confusion matrix.\n",
        "    Normalization can be applied by setting `normalize=True`.\n",
        "    \"\"\"\n",
        "    if not title:\n",
        "        if normalize:\n",
        "            title = 'Normalized confusion matrix'\n",
        "        else:\n",
        "            title = 'Confusion matrix, without normalization'\n",
        "\n",
        "    # Compute confusion matrix\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    # Only use the labels that appear in the data\n",
        "    #classes = classes[unique_labels(y_true, y_pred)]\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "        #print(\"Normalized confusion matrix\")\n",
        "    #else:\n",
        "        #print('Confusion matrix, without normalization')\n",
        "\n",
        "    #print(cm)\n",
        "\n",
        "    fig, ax = plt.subplots()\n",
        "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    ax.figure.colorbar(im, ax=ax)\n",
        "    # We want to show all ticks...\n",
        "    ax.set(xticks=np.arange(cm.shape[1]),\n",
        "           yticks=np.arange(cm.shape[0]),\n",
        "           # ... and label them with the respective list entries\n",
        "           xticklabels=classes, yticklabels=classes,\n",
        "           title=title,\n",
        "           ylabel='Ground-truth label',\n",
        "           xlabel='Predicted label')\n",
        "\n",
        "    # Rotate the tick labels and set their alignment.\n",
        "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
        "             rotation_mode=\"anchor\")\n",
        "\n",
        "    # Loop over data dimensions and create text annotations.\n",
        "    fmt = '.2f' if normalize else 'd'\n",
        "    thresh = cm.max() / 2.\n",
        "    for i in range(cm.shape[0]):\n",
        "        for j in range(cm.shape[1]):\n",
        "            ax.text(j, i, format(cm[i, j], fmt),\n",
        "                    ha=\"center\", va=\"center\",\n",
        "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "    fig.tight_layout()\n",
        "    return ax\n",
        "\n",
        "X, Y = test_generator.next() # Get the X (images) and Y (labels) of the test set\n",
        "\n",
        "labels_pred = best_model.predict(X) #predict the output from X\n",
        "\n",
        "labels_pred = labels_pred > labels_pred.mean() #predictions greater than mean are set to 1, those lesser than or equal to mean are set to 0.\n",
        "\n",
        "labels_test = Y\n",
        "\n",
        "np.set_printoptions(precision=2)\n",
        "\n",
        "# Plot non-normalized confusion matrix\n",
        "plot_confusion_matrix(labels_test, labels_pred.astype('int'), classes=['Normal','Hematoma'], normalize=False,\n",
        "                      title='Confusion matrix, without normalization')\n",
        "\n",
        "print ('\\033[1m' + 'Ready for next step!')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A-z04WjMXWLE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Plotting the ROC curve with the AUC\n",
        "\n",
        "labels_pred = best_model.predict(X) # predict again to get the original sigmoid output [0,1]\n",
        "\n",
        "from sklearn import metrics\n",
        "\n",
        "fpr, tpr, thresholds = metrics.roc_curve(labels_test, labels_pred)\n",
        "roc_auc = metrics.auc(fpr, tpr)\n",
        "\n",
        "plt.figure()\n",
        "lw = 2\n",
        "plt.plot(fpr, tpr, color='darkorange',\n",
        "         lw=lw, label='ROC curve (area = %0.2f)' % roc_auc)\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
        "plt.xlim([-0.01, 1.0])\n",
        "plt.ylim([0.0, 1.01])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver operating characteristic example')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()\n",
        "\n",
        "labels_pred = labels_pred > labels_pred.mean() #predictions greater than mean are set to 1, those lesser than or equal to mean are set to 0.\n",
        "\n",
        "f1_score = metrics.f1_score(labels_test, labels_pred, labels=None, pos_label=1, average='binary', sample_weight=None)\n",
        "\n",
        "accuracy = metrics.accuracy_score(labels_test, labels_pred)\n",
        "\n",
        "print(\"Accuracy: \" + str(accuracy))\n",
        "\n",
        "print(\"F1 Score: \" + str(f1_score))\n",
        "\n",
        "print ('\\033[1m' + 'Ready for next step!')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZJFC4Sv78DMP",
        "colab_type": "text"
      },
      "source": [
        "### 5.5 - Test Evaluation Transfer Learning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7v2NUvMT7aib",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Finally, we can use the test set for predictions\n",
        "\n",
        "test_data_dir = '/All cases/Test/' # location of test dataset\n",
        "\n",
        "test_datagen = ImageDataGenerator(\n",
        "         rescale=1./255)       # normalize pixel values to [0,1]\n",
        "\n",
        "# Preparing test set images for prediction\n",
        "\n",
        "itr = test_generator = test_datagen.flow_from_directory(\n",
        "    test_data_dir,\n",
        "    target_size=(img_height, img_width),\n",
        "    batch_size=40,\n",
        "    shuffle='False',\n",
        "    class_mode='binary')\n",
        "batch_x, batch_y = itr.next()\n",
        "\n",
        "print('Test group accuracy: ', best_model.evaluate(batch_x, batch_y, verbose=0)[1])\n",
        "\n",
        "\n",
        "\n",
        "from random import randrange\n",
        "\n",
        "prediction1 = np.round(best_model.predict(batch_x, verbose=1))==0\n",
        "\n",
        "\n",
        "start_idx = randrange(batch_x.shape[0]-10) \n",
        "fig, ax = plt.subplots(2,5, figsize=(15,8))\n",
        "for j in range(0,2): \n",
        "  for i in range(0,5):\n",
        "     ax[j][i].xaxis.set_major_locator(plt.NullLocator())\n",
        "     ax[j][i].yaxis.set_major_locator(plt.NullLocator())\n",
        "     ax[j][i].imshow(batch_x[start_idx], cmap='gray')\n",
        "     ax[j][i].set_title(\"Index:{} \\nPrediction:{}\".format(start_idx, prediction1[start_idx]))\n",
        "     start_idx +=1\n",
        "plt.show()\n",
        "\n",
        "print ('\\033[1m' + 'Ready for next step!')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TodQNrn-8jqC",
        "colab_type": "text"
      },
      "source": [
        "### 5.6 - Visualization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JU3kUp6ccpVV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Importing visualization tools\n",
        "\n",
        "from keras.preprocessing import image\n",
        "from keras.applications.inception_v3 import preprocess_input, decode_predictions\n",
        "from keras.layers import Input\n",
        "from keras import activations\n",
        "from keras.models import load_model\n",
        "from keras.layers import Dropout, Flatten, Dense, GlobalAveragePooling2D\n",
        "from keras import initializers\n",
        "from keras.models import Sequential, Model\n",
        "from vis.visualization import visualize_activation,visualize_saliency,overlay,visualize_cam\n",
        "from vis.utils import utils\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.applications import imagenet_utils\n",
        "import numpy as np\n",
        "\n",
        "print ('\\033[1m' + 'Ready for next step!')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ov0-RdexmstW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(best_model.summary())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "We-Z6sWNVEzR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "layer_idx = utils.find_layer_idx(best_model, 'block5_conv3')\n",
        "print(\"Remove Activation from Last Layer\")\n",
        "# Swap softmax with linear\n",
        "best_model.layers[layer_idx].activation = activations.linear\n",
        "print(\"Done. Now Applying changes to the model ...\")\n",
        "activation2_model = utils.apply_modifications(best_model)\n",
        "print ('\\033[1m' + 'Ready for next step!')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1-uKEIf9Vd-T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#print(activation_model.summary())\n",
        "#im_files=[\"/All cases/Test/Hematoma/Test hematoma (1).png\",\"/All cases/Test/Normal/Test_normal (1).png\"]\n",
        "import os\n",
        "cwd = os.getcwd()\n",
        "import os\n",
        "\n",
        "dir_name='/All cases/Test/'\n",
        "im_files = test_generator.filenames\n",
        "for im_file in im_files[:3]:\n",
        "    img1 = image.load_img(dir_name + im_file,target_size=(150,150))\n",
        "    img1 = image.img_to_array(img1)\n",
        "    img1 = np.expand_dims(img1, axis=0)\n",
        "    img1 = preprocess_input(img1)\n",
        "    layer_idx = utils.find_layer_idx(activation2_model, 'block5_conv3')\n",
        "    heatmap = visualize_cam(activation2_model, layer_idx, filter_indices=range(activation2_model.layers[layer_idx].filters), seed_input=img1[0,:,:,:])\n",
        "    img_init=utils.load_img(dir_name + im_file,target_size=(150,150))\n",
        "    img_init = img_init[:,:,:3]\n",
        "    plt.figure(figsize=(20,20))\n",
        "    ax1 = plt.subplot(1,3,1)\n",
        "    ax1.grid(False)\n",
        "    plt.imshow(img_init, cmap='gray')\n",
        "    ax2 = plt.subplot(1,3,2)\n",
        "    ax2.grid(False)\n",
        "    plt.imshow(heatmap)\n",
        "    ax3 = plt.subplot(1,3,3)\n",
        "    ax3.grid(False)\n",
        "    plt.imshow(overlay(img_init, heatmap))\n",
        "    plt.show()\n",
        "    \n",
        "print ('\\n' + '\\033[1m' + 'Congratulations, you have completed the assignment!')\n",
        "\n",
        "from IPython.display import HTML\n",
        "HTML('<img src=\"https://media.giphy.com/media/cub3pntkz8muQ/giphy.gif\">')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}